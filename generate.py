import os
import torch
import torchaudio
import numpy as np
import matplotlib.pyplot as plt
from matcha.models.matcha_tts import MatchaTTS
from matcha.text_to_ID.text_to_sequence import text_to_sequence

# --- CONFIGURATION ---
CHECKPOINT_PATH = None  # Laissera le script trouver le dernier automatiquement
OUTPUT_FOLDER = "generated_audio"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
TEXTE_A_DIRE = "Hello, I am your Matcha Text to Speech model, what can I do for you."

def get_latest_checkpoint(logs_dir="lightning_logs"):
    """Trouve automatiquement le dernier fichier .ckpt pour avoir les derniers poids."""
    import glob
    # Cherche rÃ©cursivement tous les .ckpt
    files = glob.glob(f"{logs_dir}/**/*.ckpt", recursive=True)
    if not files:
        raise FileNotFoundError("Aucun checkpoint trouvÃ© ! As-tu lancÃ© l'entraÃ®nement ?")
    # Trie par date de modification (le plus rÃ©cent en dernier)
    latest_file = max(files, key=os.path.getmtime)
    print(f"âœ… Checkpoint trouvÃ© : {latest_file}")
    return latest_file

def simple_euler_ode_solver(model, mu, n_steps=10):
    """
    Le cÅ“ur du Flow Matching : transforme le bruit en son pas Ã  pas.
    """
    # 1. On part d'un bruit blanc (t=0)
    # mu shape: [1, 80, T]
    z = torch.randn_like(mu, device=DEVICE)
    
    # 2. On avance dans le temps de 0 Ã  1
    dt = 1.0 / n_steps
    
    print(f"ğŸ”„ GÃ©nÃ©ration en {n_steps} Ã©tapes...")
    
    for i in range(n_steps):
        t_val = i / n_steps
        t = torch.tensor([t_val], device=DEVICE)
        
        # Le dÃ©codeur prÃ©dit la vitesse (le vecteur direction)
        # On n'a pas besoin de masque ici car on gÃ©nÃ¨re tout
        v_pred = model.decoder(z, t, mu, mask=None)
        
        # Euler step : nouvelle position = ancienne + vitesse * temps
        z = z + v_pred * dt
        
    return z # C'est notre spectrogramme gÃ©nÃ©rÃ© (y_hat)

def main():
    os.makedirs(OUTPUT_FOLDER, exist_ok=True)

    # 1. Chargement du modÃ¨le
    ckpt = CHECKPOINT_PATH if CHECKPOINT_PATH else get_latest_checkpoint()
    print("â³ Chargement du modÃ¨le...")
    
    # On charge le modÃ¨le et ses hyperparamÃ¨tres
    model = MatchaTTS.load_from_checkpoint(ckpt)
    model.to(DEVICE)
    model.eval() # Mode Ã©valuation (dÃ©sactive le dropout)

    # 2. PrÃ©paration du texte
    print(f"ğŸ“– Texte : '{TEXTE_A_DIRE}'")
    sequence = text_to_sequence(TEXTE_A_DIRE, ["english_cleaners"]) # Ou basic_cleaners
    x = torch.tensor([sequence], dtype=torch.long, device=DEVICE)
    x_lengths = torch.tensor([len(sequence)], dtype=torch.long, device=DEVICE)

    with torch.no_grad():
        # 3. Utilisation de la mÃ©thode synthesise du modÃ¨le pour le processus d'infÃ©rence complet
        # Cela gÃ¨re automatiquement l'encodage, l'alignement et la gÃ©nÃ©ration
        output = model.synthesise(
            x=x,
            x_lengths=x_lengths,
            n_timesteps=50,
            temperature=1.0,
            length_scale=1.0
        )
        
        # RÃ©cupÃ©ration du mel spectrogramme gÃ©nÃ©rÃ©
        spectrogram = output["decoder_outputs"]  # C'est le mel dÃ©jÃ  dÃ©normalisÃ©

    # 5. Conversion Spectrogramme -> Audio (Griffin-Lim)
    # C'est une mÃ©thode mathÃ©matique pour reconstruire le son sans Vocoder entraÃ®nÃ©
    # 5. Conversion Spectrogramme -> Audio (Inverse Mel + Griffin-Lim)
    print("ğŸ”Š Conversion en audio (InvMel -> Griffin-Lim)...")
    
    # A. CrÃ©ation de la transformation Inverse Mel (Pour passer de 80 -> 513 canaux)
    # On doit utiliser les mÃªmes paramÃ¨tres que ceux utilisÃ©s pour crÃ©er le dataset LJSpeech
    inv_mel_scale = torchaudio.transforms.InverseMelScale(
        n_stft=1024 // 2 + 1,  # = 513 bins de frÃ©quence
        n_mels=80,
        sample_rate=22050,
        f_min=0.0,
        f_max=8000.0,
        norm='slaney',
        mel_scale='slaney' 
    ).to(DEVICE)

    # B. Configuration de Griffin-Lim (Pour passer de Spectrogramme -> Onde)
    griffin_lim = torchaudio.transforms.GriffinLim(
        n_fft=1024, 
        n_iter=32, 
        hop_length=256,
        win_length=1024,
        power=1.0
    ).to(DEVICE)
    
    # C. ExÃ©cution du Pipeline
    # 1. Le modÃ¨le sort dÃ©jÃ  des mels (synthesise retourne le mel dÃ©jÃ  dÃ©normalisÃ©)
    # Utiliser output["mel"] si disponible, sinon utiliser decoder_outputs
    if "mel" in output:
        mel_spectrogram = output["mel"]  # DÃ©jÃ  dÃ©normalisÃ©
    else:
        # Si on a seulement decoder_outputs, il faudra peut-Ãªtre dÃ©normaliser
        mel_spectrogram = spectrogram
    
    # S'assurer que mel_spectrogram est positif (si c'est un log-mel, il faut exp)
    if mel_spectrogram.min() < 0:
        mel_spectrogram = torch.exp(mel_spectrogram)
    
    # 2. On "dÃ©compresse" : Mel (80) -> LinÃ©aire (513)
    linear_spectrogram = inv_mel_scale(mel_spectrogram)
    
    # 3. On reconstruit la phase et l'onde sonore
    waveform = griffin_lim(linear_spectrogram)

    # 6. Sauvegarde
    save_path = os.path.join(OUTPUT_FOLDER, "test_matcha.wav")
    torchaudio.save(save_path, waveform.cpu(), sample_rate=22050)
    print(f"âœ¨ Audio sauvegardÃ© dans : {save_path}")

    # (Optionnel) Afficher le spectrogramme
    plot_data = mel_spectrogram.squeeze().cpu().numpy()
    
    # æ‰“å°è°ƒè¯•ä¿¡æ¯
    print(f"ğŸ“Š Mel spectrogram ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   Min: {plot_data.min():.4f}, Max: {plot_data.max():.4f}")
    print(f"   Mean: {plot_data.mean():.4f}, Std: {plot_data.std():.4f}")
    
    # å¦‚æœå€¼æœ‰è´Ÿæ•°ï¼Œè¯´æ˜è¿˜åœ¨logç©ºé—´ï¼Œéœ€è¦exp
    if plot_data.min() < 0:
        print("   æ£€æµ‹åˆ°è´Ÿå€¼ï¼Œåº”ç”¨expå˜æ¢...")
        plot_data = np.exp(plot_data)
        print(f"   Expå - Min: {plot_data.min():.4f}, Max: {plot_data.max():.4f}")
    
    # ä½¿ç”¨dB scaleï¼ˆåˆ†è´åˆ»åº¦ï¼‰æ¥å¢å¼ºå¯¹æ¯”åº¦ - è¿™æ˜¯è®ºæ–‡ä¸­å¸¸ç”¨çš„æ–¹æ³•
    # dB = 20 * log10(value)ï¼Œä½†éœ€è¦é¿å…log(0)
    eps = 1e-10  # é¿å…log(0)
    plot_data_db = 20 * np.log10(plot_data + eps)
    
    # è®¾ç½®åˆç†çš„dBèŒƒå›´ï¼ˆé€šå¸¸-80dBåˆ°0dBæˆ–æ›´é«˜ï¼‰
    db_min = np.percentile(plot_data_db, 1)  # è£å‰ªæä½å€¼
    db_max = np.percentile(plot_data_db, 99)  # è£å‰ªæé«˜å€¼
    # å¦‚æœdb_maxå¤ªå°ï¼Œä½¿ç”¨å®é™…æœ€å¤§å€¼
    if db_max < -10:
        db_max = plot_data_db.max()
    
    print(f"   dBèŒƒå›´: {db_min:.2f} dB åˆ° {db_max:.2f} dB")
    
    # ä¿å­˜mel spectrogram (ä½¿ç”¨dB scale)
    plt.figure(figsize=(12, 6))
    img = plt.imshow(plot_data_db, origin='lower', aspect='auto', cmap='viridis',
                     vmin=db_min, vmax=db_max, interpolation='bilinear')
    plt.title("Mel Spectrogramme GÃ©nÃ©rÃ©")
    plt.xlabel("Time (Frames)")
    plt.ylabel("Mel Frequency Bins")
    cbar = plt.colorbar(img, label='Intensity (dB)')
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_FOLDER, "mel_spectrogram.png"), dpi=150, bbox_inches='tight')
    print("ğŸ“Š Mel Spectrogramme sauvegardÃ© (ä½¿ç”¨dBåˆ»åº¦ï¼Œåº”è¯¥æ›´äº®äº†ï¼).")

if __name__ == "__main__":
    main()