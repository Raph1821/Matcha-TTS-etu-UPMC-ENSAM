import os
import torch
import torchaudio
import numpy as np
import matplotlib.pyplot as plt
from matcha.models.matcha_tts import MatchaTTS
from matcha.text_to_ID.text_to_sequence import text_to_sequence

# --- CONFIGURATION ---
CHECKPOINT_PATH = None  # Laissera le script trouver le dernier automatiquement
OUTPUT_FOLDER = "generated_audio"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
TEXTE_A_DIRE = "Hello, I am your Matcha Text to Speech model, what can I do for you."

def get_latest_checkpoint(logs_dir="lightning_logs"):
    """Trouve automatiquement le dernier fichier .ckpt pour avoir les derniers poids."""
    import glob
    # Cherche rÃ©cursivement tous les .ckpt
    files = glob.glob(f"{logs_dir}/**/*.ckpt", recursive=True)
    if not files:
        raise FileNotFoundError("Aucun checkpoint trouvÃ© ! As-tu lancÃ© l'entraÃ®nement ?")
    # Trie par date de modification (le plus rÃ©cent en dernier)
    latest_file = max(files, key=os.path.getmtime)
    print(f"âœ… Checkpoint trouvÃ© : {latest_file}")
    return latest_file

def simple_euler_ode_solver(model, mu, n_steps=10):
    """
    Le cÅ“ur du Flow Matching : transforme le bruit en son pas Ã  pas.
    """
    # 1. On part d'un bruit blanc (t=0)
    # mu shape: [1, 80, T]
    z = torch.randn_like(mu, device=DEVICE)
    
    # 2. On avance dans le temps de 0 Ã  1
    dt = 1.0 / n_steps
    
    print(f"ğŸ”„ GÃ©nÃ©ration en {n_steps} Ã©tapes...")
    
    for i in range(n_steps):
        t_val = i / n_steps
        t = torch.tensor([t_val], device=DEVICE)
        
        # Le dÃ©codeur prÃ©dit la vitesse (le vecteur direction)
        # On n'a pas besoin de masque ici car on gÃ©nÃ¨re tout
        v_pred = model.decoder(z, t, mu, mask=None)
        
        # Euler step : nouvelle position = ancienne + vitesse * temps
        z = z + v_pred * dt
        
    return z # C'est notre spectrogramme gÃ©nÃ©rÃ© (y_hat)

def main():
    os.makedirs(OUTPUT_FOLDER, exist_ok=True)

    # 1. Chargement du modÃ¨le
    ckpt = CHECKPOINT_PATH if CHECKPOINT_PATH else get_latest_checkpoint()
    print("â³ Chargement du modÃ¨le...")
    
    # On charge le modÃ¨le et ses hyperparamÃ¨tres
    model = MatchaTTS.load_from_checkpoint(ckpt)
    model.to(DEVICE)
    model.eval() # Mode Ã©valuation (dÃ©sactive le dropout)

    # 2. PrÃ©paration du texte
    print(f"ğŸ“– Texte : '{TEXTE_A_DIRE}'")
    sequence = text_to_sequence(TEXTE_A_DIRE, ["english_cleaners"]) # Ou basic_cleaners
    x = torch.tensor([sequence], dtype=torch.long, device=DEVICE)
    x_lengths = torch.tensor([len(sequence)], dtype=torch.long, device=DEVICE)

    with torch.no_grad():
        # 3. Utilisation de la mÃ©thode synthesise du modÃ¨le pour le processus d'infÃ©rence complet
        # Cela gÃ¨re automatiquement l'encodage, l'alignement et la gÃ©nÃ©ration
        output = model.synthesise(
            x=x,
            x_lengths=x_lengths,
            n_timesteps=50,
            temperature=1.0,
            length_scale=1.0
        )
        
        # RÃ©cupÃ©ration du mel spectrogramme gÃ©nÃ©rÃ©
        spectrogram = output["decoder_outputs"]  # C'est le mel dÃ©jÃ  dÃ©normalisÃ©

    # 5. Conversion Spectrogramme -> Audio (Griffin-Lim)
    # C'est une mÃ©thode mathÃ©matique pour reconstruire le son sans Vocoder entraÃ®nÃ©
    # 5. Conversion Spectrogramme -> Audio (Inverse Mel + Griffin-Lim)
    print("ğŸ”Š Conversion en audio (InvMel -> Griffin-Lim)...")
    
    # A. CrÃ©ation de la transformation Inverse Mel (Pour passer de 80 -> 513 canaux)
    # On doit utiliser les mÃªmes paramÃ¨tres que ceux utilisÃ©s pour crÃ©er le dataset LJSpeech
    inv_mel_scale = torchaudio.transforms.InverseMelScale(
        n_stft=1024 // 2 + 1,  # = 513 bins de frÃ©quence
        n_mels=80,
        sample_rate=22050,
        f_min=0.0,
        f_max=8000.0,
        norm='slaney',
        mel_scale='slaney' 
    ).to(DEVICE)

    # B. Configuration de Griffin-Lim (Pour passer de Spectrogramme -> Onde)
    griffin_lim = torchaudio.transforms.GriffinLim(
        n_fft=1024, 
        n_iter=32, 
        hop_length=256,
        win_length=1024,
        power=1.0
    ).to(DEVICE)
    
    # C. ExÃ©cution du Pipeline
    # 1. Le modÃ¨le sort dÃ©jÃ  des mels (synthesise retourne le mel dÃ©jÃ  dÃ©normalisÃ©)
    # Utiliser output["mel"] si disponible, sinon utiliser decoder_outputs
    if "mel" in output:
        mel_spectrogram = output["mel"]  # DÃ©jÃ  dÃ©normalisÃ©
    else:
        # Si on a seulement decoder_outputs, il faudra peut-Ãªtre dÃ©normaliser
        mel_spectrogram = spectrogram
    
    # S'assurer que mel_spectrogram est positif (si c'est un log-mel, il faut exp)
    if mel_spectrogram.min() < 0:
        mel_spectrogram = torch.exp(mel_spectrogram)
    
    # 2. On "dÃ©compresse" : Mel (80) -> LinÃ©aire (513)
    linear_spectrogram = inv_mel_scale(mel_spectrogram)
    
    # 3. On reconstruit la phase et l'onde sonore
    waveform = griffin_lim(linear_spectrogram)

    # 6. Sauvegarde
    save_path = os.path.join(OUTPUT_FOLDER, "test_matcha.wav")
    torchaudio.save(save_path, waveform.cpu(), sample_rate=22050)
    print(f"âœ¨ Audio sauvegardÃ© dans : {save_path}")

    # (Optionnel) Afficher le spectrogramme
    # æ³¨æ„ï¼šæ¨¡å‹è¾“å‡ºçš„æ˜¯log-mel spectrogramï¼ˆåœ¨logç©ºé—´ï¼‰ï¼Œéœ€è¦expæ‰èƒ½å¾—åˆ°çº¿æ€§mel
    # è¿™æ ·å¯è§†åŒ–ä¼šæ›´äº®ï¼Œæ›´æ¥è¿‘è®ºæ–‡ä¸­çš„æ•ˆæœ
    plot_data_log = mel_spectrogram.squeeze().cpu().numpy()
    
    # è½¬æ¢ä¸ºçº¿æ€§melï¼ˆexpå˜æ¢ï¼‰ï¼Œè¿™æ ·å¯è§†åŒ–ä¼šæ›´äº®
    plot_data_linear = np.exp(plot_data_log)
    
    # ä¿å­˜çº¿æ€§mel spectrogramï¼ˆexpåï¼Œæ›´äº®ï¼Œæ›´æ¥è¿‘è®ºæ–‡æ•ˆæœï¼‰
    plt.figure(figsize=(12, 6))
    # è°ƒæ•´vminå’Œvmaxä»¥æ›´å¥½åœ°æ˜¾ç¤ºçº¿æ€§melçš„èŒƒå›´
    vmin_linear = np.percentile(plot_data_linear, 1)
    vmax_linear = np.percentile(plot_data_linear, 99)
    plt.imshow(plot_data_linear, origin='lower', aspect='auto', cmap='viridis',
               vmin=vmin_linear, vmax=vmax_linear)
    plt.title("Mel Spectrogramme GÃ©nÃ©rÃ©")
    plt.xlabel("Time (Frames)")
    plt.ylabel("Mel Frequency Bins")
    plt.colorbar(label='Intensity')
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_FOLDER, "mel_spectrogram.png"), dpi=150)
    print("ğŸ“Š Mel Spectrogramme sauvegardÃ©.")

if __name__ == "__main__":
    main()